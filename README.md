# Adversarial Attacks on Images by Relighting

Find artificial relightings that fool classifiers.

## Project Description
- perform research on the field of available image augmentation techniques that simulate different illumination
- adapt the most suitable frameworks to attack the classifier
- evaluate the robustness of the targeted model against this type of input pertubations
- analyze whether the adversarial training approach helps improving its robustness.

## Repo Structure
Attacks: this package contains all our adversarial attack algorithms <br>
Classifiers: this package contains all classifiers that we are attacking in our experiments <br>
Data: contains data we train the classifiers on <br>
Dep: deprecated notebooks and scripts <br>
Experiments: notebooks that execute our adversarial attack experiments <br>
Relighters: model implementations of all relighters <br>

## Contributors

- [Johanna](https://github.com/johannaSommer)
- [Pavel](https://github.com/PavelCz)
- [Andreea](https://github.com/AndreeaMusat)
- Thank you to our advisor Aleksei!
