{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classifying Celebrity Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we use the 10 identities of the PubFig dataset: <br>\n",
    "https://www.cs.columbia.edu/CAVE/databases/pubfig/ <br>\n",
    "We roughly follow the structure of this\n",
    "tutorial:<br>\n",
    "https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/ <br>\n",
    "And use the FaceNet architecture from this repository: <br>\n",
    "https://github.com/timesler/facenet-pytorch <br>\n",
    "This architecture was introduced in: <br>\n",
    "https://arxiv.org/pdf/1503.03832.pdf <br>\n",
    "We use the MTCNN to crop input images so that they only depict the face. We then feed this into the InceptionResnet to get \n",
    "embeddings for each face. Lastly, we classify those embeddings with a simple NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/src/classifiers/FaceNet\n",
      "/home/jupyter/src\n",
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "os.chdir('/home/jupyter/src')\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from classifiers.FaceNet.Facenet import crop_images_batch, encode_pubfig\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load dataset, crop images, get embeddings and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop, labels = crop_images_batch(device, '/home/jupyter/src/data/pubfig83', postprocess=True)\n",
    "np.savez_compressed('/home/jupyter/src/data/pubfig83/pubfig83_crop.npz', crop=crop, labels=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13801, 3, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "#import random\n",
    "#crop = np.load('/home/jupyter/src/data/pubfig83/pubfig83_crop.npz')['crop']\n",
    "#print(crop.shape)\n",
    "#y_train = np.load('/home/jupyter/src/data/pubfig83/pubfig83_crop.npz')['labels']\n",
    "#test = []\n",
    "#test_labels = []\n",
    "#train = []\n",
    "#train_labels = []\n",
    "#random.seed(a=0)\n",
    "#for id, elem in enumerate(crop):\n",
    "#    rand = random.random()\n",
    "#    if rand > 0.95:\n",
    "#        test.append(elem)\n",
    "#        test_labels.append(y_train[id])\n",
    "#    else:\n",
    "#        train.append(elem)\n",
    "#        train_labels.append(y_train[id])\n",
    "#np.savez_compressed('/home/jupyter/src/data/pubfig83/pubfig83_crop_test.npz', data=test, labels=test_labels)\n",
    "#np.savez_compressed('/home/jupyter/src/data/pubfig83/pubfig83_crop_train.npz', data=train, labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "embeddings = []\n",
    "for image in train_crop:\n",
    "    embedding = resnet(torch.Tensor(np.expand_dims(image, axis=0))).detach().numpy()\n",
    "    embeddings.append(embedding)\n",
    "y_train = np.load('/home/jupyter/src/data/pubfig83/pubfig83_crop_train.npz')['labels']\n",
    "np.savez_compressed('/home/jupyter/src/data/pubfig83/pubfig83_embedding.npz', X_train=embeddings, y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read in all embeddings and labels and normalize them to an L2 norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "path = '../../../data/PubFig/embedding'\n",
    "embeddings = None\n",
    "labels = None\n",
    "for (dirpath, dirnames, filenames) in walk(path):\n",
    "    for f in filenames:\n",
    "        read_em = np.load(os.path.join(path, f))['X_train']\n",
    "        read_label = np.load(os.path.join(path, f))['y_train']\n",
    "        if embeddings is None and labels is None:\n",
    "            embeddings = read_em\n",
    "            labels = read_label\n",
    "        else:\n",
    "            embeddings = np.concatenate((embeddings, read_em), axis=0)\n",
    "            labels = np.concatenate((labels, read_label))\n",
    "print(embeddings.shape)\n",
    "print(labels.shape)\n",
    "embeddings = torch.nn.functional.normalize(torch.Tensor(embeddings), p=2, dim=1, eps=1e-12, out=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13152\n",
      "512\n",
      "13152\n"
     ]
    }
   ],
   "source": [
    "path = 'data/PubFig/embedding'\n",
    "embeddings_old = np.load('/home/jupyter/src/data/pubfig83/pubfig83_embedding.npz')['X_train']\n",
    "embeddings = []\n",
    "for em in embeddings_old:\n",
    "    embeddings.append(em[0])\n",
    "labels = np.load('/home/jupyter/src/data/pubfig83/pubfig83_embedding.npz')['y_train']\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n",
    "print(len(labels))\n",
    "embeddings = torch.nn.functional.normalize(torch.Tensor(embeddings), p=2, dim=1, eps=1e-12, out=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fit the 1-Layer NN to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from classifiers.FaceNet.Facenet import FaceNet\n",
    "classifier = FaceNet(num_classes=83, load_model=False)\n",
    "loss_histor = classifier.train(torch.Tensor(embeddings), torch.Tensor(labels), num_steps=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and test the network on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "_img = np.array(Image.open('../../../data/testimg/Aaron-Eckhart/images.jpg'))\n",
    "\n",
    "plt.imshow(_img)\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")\n",
    "x_aligned, prob = mtcnn(_img, return_prob=True)\n",
    "plt.imshow(np.swapaxes(np.swapaxes(x_aligned, 0, 2), 0, 1))\n",
    "\n",
    "output = classifier.predict(x_aligned.unsqueeze(0))\n",
    "_, orig_label = torch.max(output, dim=1)\n",
    "print('prediction: ', orig_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load test images that we collected ourselves (4 images for each identity) and evaluate\n",
    "the network performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_crops = \n",
    "test_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n"
     ]
    }
   ],
   "source": [
    "crop = np.load('/home/jupyter/src/data/pubfig83/pubfig83_crop_test.npz')['data']\n",
    "labels =  np.load('/home/jupyter/src/data/pubfig83/pubfig83_crop_test.npz')['labels']\n",
    "crops_normalized = []\n",
    "for image in crop:\n",
    "    normalized_image = (image - 0.5) / 0.50196078\n",
    "    crops_normalized.append(normalized_image) \n",
    "\n",
    "output = []\n",
    "for id, crop_image in enumerate(crops_normalized):\n",
    "    print(id)\n",
    "    output.append(classifier.predict(torch.Tensor(crop_image).unsqueeze(0)))\n",
    "_, pred = torch.max(output, dim=1)\n",
    "\n",
    "correct = 100*(pred == labels).sum()/len(labels)\n",
    "print('Accuracy in %: ', correct.item())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
